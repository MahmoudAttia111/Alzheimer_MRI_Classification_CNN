# -*- coding: utf-8 -*-
"""task3_Alzheimer_Classification_(CNN).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f9aIG4CaTs2T-R7NvUlag56qAC8aIR50
"""

!pip install opendatasets
!pip install split-folders
# !pip install tensorflow

import opendatasets as od
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow.keras import optimizers
from tensorflow import keras
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import confusion_matrix,classification_report

"""### write any name or any passoward >>name: hfdd, password 343433"""

od.download("https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset")

import splitfolders

splitfolders.ratio(
    "/content/augmented-alzheimer-mri-dataset/AugmentedAlzheimerDataset",
    output="/content/alzheimer_split",
    seed=42,
    ratio=(0.8, 0.2)
)

train_ds = tf.keras.utils.image_dataset_from_directory(
    "/content/alzheimer_split/train",
    image_size=(224,224),
    batch_size=32
)
val_ds = tf.keras.utils.image_dataset_from_directory(
    "/content/alzheimer_split/val",
    image_size=(224,224),
    batch_size=32
)

# train_ds = tf.keras.utils.image_dataset_from_directory(
#     "/content/augmented-alzheimer-mri-dataset/AugmentedAlzheimerDataset",
#  validation_split = 0.2,subset = "training",seed=42,image_size=(224,224),batch_size=32)

# val_ds = tf.keras.utils.image_dataset_from_directory(
#     "/content/augmented-alzheimer-mri-dataset/AugmentedAlzheimerDataset",
#  validation_split = 0.2,subset = "validation",seed=42,image_size=(224,224),batch_size=32)

class_names = train_ds.class_names
print(class_names)

for images,lagels in train_ds.take(1):
  plt.figure(figsize=(10,10))
  for i in range(9):
    ax = plt.subplot(3,3,i+1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[lagels[i]])
    plt.axis("off")

normalization_layer = tf.keras.layers.Rescaling(1./255)
autotune = tf.data.AUTOTUNE
train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)).prefetch(buffer_size=autotune)
val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)).prefetch(buffer_size=autotune)
print(class_names)

earlyS = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss", patience=7, restore_best_weights=True
)

model = models.Sequential([
    layers.Conv2D(32,(3,3),activation="relu",padding="same",input_shape=(224,224,3)),
    layers.MaxPooling2D((2,2)),

    layers.Conv2D(64,(3,3),activation="relu",padding="same"),
    layers.MaxPooling2D((2,2)),

    layers.Conv2D(128,(3,3),activation="relu",padding="same"),
    layers.MaxPooling2D((2,2)),

    layers.Conv2D(256,(3,3),activation="relu",padding="same"),
    layers.MaxPooling2D((2,2)),

    layers.Flatten(),
    layers.Dense(256, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(4, activation="softmax")
])

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-4),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=[earlyS]
)

loss, acc = model.evaluate(val_ds)
print(f"Validation accuracy = {acc*100:.2f}%")

plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.legend()
plt.title("Accuracy")
plt.show()

# Ø¹Ø±Ø¶ Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„Ù€ Loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title("Loss")
plt.show()

test_ds = tf.keras.utils.image_dataset_from_directory(
    "/content/augmented-alzheimer-mri-dataset/OriginalDataset",
    image_size=(224,224),
    batch_size=32
).map(lambda x,y: (normalization_layer(x), y))

test_loss, test_acc = model.evaluate(test_ds)
print(f"ðŸŽ¯ Test Accuracy on Original Data: {test_acc*100:.2f}%")

model.save("/content/alzheimer_model.h5")

base_model = tf.keras.applications.ResNet50(
    include_top=False,
    weights="imagenet",
    input_shape=(224,224,3)
)

base_model.trainable = False

# âœ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
model = models.Sequential([
    layers.Input(shape=(224,224,3)),
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dropout(0.4),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.3),
    layers.Dense(4, activation="softmax")
])


model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-4),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

earlyS = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    patience=7,
    restore_best_weights=True
)

history1 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=[earlyS]
)

base_model.trainable = True
for layer in base_model.layers[:-50]:
    layer.trainable = False

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-5),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)


history2 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=[earlyS]
)

def combine_history(h1, h2):
    history = {}
    for k in h1.history.keys():
        history[k] = h1.history[k] + h2.history[k]
    return history
full_history = combine_history(history1, history2)


plt.figure(figsize=(12,5))


plt.subplot(1,2,1)
plt.plot(full_history['accuracy'], label='Train Acc')
plt.plot(full_history['val_accuracy'], label='Val Acc')
plt.title('Training & Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1,2,2)
plt.plot(full_history['loss'], label='Train Loss')
plt.plot(full_history['val_loss'], label='Val Loss')
plt.title('Training & Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

"""to install requirement run this code"""

!pip freeze > requirements.txt
from google.colab import files
files.download("requirements.txt")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile requirements.txt
# streamlit
# tensorflow
# numpy
# pillow
# from google.colab import files
# files.download("requirements.txt")
#